{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6a6fa8",
   "metadata": {},
   "source": [
    "This <b>AsteXT</b> tutorial is meant for fine-tuning our own BERT language model to detect soft NER in Asian American short stories. This tutorial is used for the AsteXT team during Fall 2025 semester.\n",
    "\n",
    "<b>Objective</b>: This tutorial will guide you in fine-tuning a BERT uncased model to classify a word into 6 categories (5 Soft NER categories and one for non-soft NER). You will get to learn topics like hyperparameter, data imbalance (and how to address it), splitting data into training and validation set, how to validate a model, and other topics. In the end, you should have a working fine-tuned BERT model that can identify soft NER from an unseen literary text.\n",
    "\n",
    "Before starting this tutorial, you should have these files ready:\n",
    "\n",
    "- A JSON file with labeled soft NER, hard NER, and general words of all the stories that you've labeled. This JSON file should be generated from \"trainingDataBuilder.py\"\n",
    "    - Note that, for this tutorial, we are using standard NER labeled, not the BIO format.\n",
    "- A JSON file with testing data from a short story that you have not labeled. This JSON file should be generated from \"testingDataBuilder.py\"\n",
    "\n",
    "You also need these libraries installed:\n",
    "`pytorch`, `numpy`, `scikit-learn`, `transformers`\n",
    "\n",
    "\n",
    "We will be using GPU to train this model. If your local laptop does not have GPU, that is okay (however, it will just take you longer). The difference in time needed between using GPU and CPU is anywhere between 10 to 100 times (as in, depending on the quality of your laptop, your built-in GPU, and your CPU, using GPU might be between 10 to 100 times faster in training than using CPU).\n",
    "\n",
    "<b>Note</b> that, traditionally, machine learning requires a test data that is also labeled to compare the fine-tuned model result against. For our case (in consideration of our data and time constraint), we do not need to label stories just to test the model since, after the trained model labels our testing data, we can manually check the model's work a posteriori.\n",
    "\n",
    "<b>Another note</b>: it is expected that you will be running this tutorial code and model training code multiple times using different hyperparameters (defined later on) or different metrics. People seldom get a good model result on their first try. Do note that, although there are a lot of guardrails that we can implement into the training process to ensure a higher quality, the Number One most influential factor in the quality of your model is not the metrics or parameters we will be using but the data that you have labeled yourself. The better your label and the more you label, the better your model (most of the time).\n",
    "\n",
    "When you go through this tutorial, make sure you read <b>all of my code comments throughout a code cell and written notes in Markdown cells in between code cells</b> (<--- again, important), as these will provide you with explanations about what is happening at each step as well as how you can interpret your model.\n",
    "\n",
    "You do not need to modify each code cell. The code cells with my comment \"Run without modifying\" or something along that line on the top does not need to be modified. You can just run it. Some code cells have places that you need to make modifications to. I have labeled those with `# TODO`. Note that within one code cell, there might be more than one `TODO`s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac231d",
   "metadata": {},
   "source": [
    "---\n",
    "Heads-up:\n",
    "\n",
    "As you go through the tutorial, there are a few variables that you need to modify yourself (such as model hyperparameters, data balance ratios, etc.) that do not have one correct answer about what value to use, as they are architectural designs that vary from person to person and data to data (especially when each of us have labeled our own stories and are thus using different stories). In other words, it is expected that you will be running this tutorial at least two times, testing out different parameters or values. To help you keep track of what values you've used, here's a list of all the values that you will be testing out in this tutorial (I might have missed one or two here and there, so please read the code carefully. I think I have included everything in this list):\n",
    "\n",
    "In the function `balanceDatasetByCategory` (for increasing underrepresented data points (most likely soft NER) in our data to mitigate data imbalance):\n",
    "- `targetRatio` (integer, target ratio between positive:negative data, e.g. = 10. I recommend keeping this ratio between 5-15. You are free to experiment outside of this range.)\n",
    "- `boostRareCategories` (boolean, since not all soft NER data categories have the same amount of data, should we particularly increase those soft NER categories that have particularly less data? E.g. = True/False)\n",
    "- `maxRepeatPerSentence` (integer, the maximum time that a lesser represented data point could be repeated, e.g. = 30. I recommend keeping this ratio between 10-40. You are free to experiment outside of this range.) \n",
    "- `probabilistic` (boolean, whether, during data upsampling, we randomly select data points to upsample or not. I highly recommend putting this to True) \n",
    "\n",
    "In the function `calculateClassWeights` (for calculating data weights in training to mitigage data imbalance):\n",
    "- `minCount` (integer, the minimum number a data should appear. I recommend between 1-5. You are free to experiment outside of this range.) \n",
    "- `clipRatio` (integer, a ratio representing the largest ratio difference in weights, e.g.: when we give extra weight to less represented data, we cannot infinitely increase the weight, since that would cause errors. The ratio says what's the largest degree of weight we could increase for underrepresented data. I recommend between 5.0-15.0. You are free to experiment outside of this range)\n",
    "\n",
    "In the function `makeValidationSplitByCategory` (for creating validation and training data split from your labeled data):\n",
    "- `validationProportion` (float, the proportion of labeled data that you want to set aside for validation. I set it to 0.05, which is a good place to start, but you are free to change ot if you want to. Usually, this number should be below 25%, or 0.25. Since we are working with a small labeled dataset, I would say less than 15%, or 0.15)\n",
    "\n",
    "Before the main model training iteration, we have these two hyperparameters:\n",
    "- `trainingSteps` (integer, number of training steps the model should go through. The higher the number, the more training the model does, and more closely aligned with the training data the model will be. If the model gets too close, the model will overfit. I recommend between 300-1200, based on your performance)\n",
    "- `learningRate` (float, learning rate during training. I recommend one of these: `3e-5, 1e-5, 5e-6, 3e-6, 1e-6, 5e-7`)\n",
    "\n",
    "A hyperparameter is a value used in model training that is resulted from architectural choices. Meaning, there is no one set of hyperparameters that suits everyone. It depends on the model you are training, your data, and many other factors. There are more than 2 hyperparameters in machine learning, but we will focus on these two.\n",
    "\n",
    "It is expected for you to try out different `trainingSteps` and `learningRate` parameters, since you are likely need to train multiple times to find a reliable model. Each time you train, try a different value. I have more detailed description of these two hyperparameters later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32229419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will download the libraries that we need:\n",
    "!pip install torch numpy scikit-learn transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we will import these libraries \n",
    "# Run without modifying\n",
    "import json, torch, random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14636985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: this is to determine whether you have GPU or CPU\n",
    "\n",
    "def getTrainingDevice():\n",
    "    '''This function determines which type of GPU you have and store it in the variable \"device\" for future use in this tutorial. If you do not have a GPU, the function will print \"Using CPU (training will be slower)\"'''\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# Save our GPU or CPU preference here for later use\n",
    "device = getTrainingDevice()\n",
    "print(f\"You are using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfece0",
   "metadata": {},
   "source": [
    "If the above printed statement says either \"You are using: mps\" or \"You are using: cuda\", then you have GPU available to use in your local computer. If it says \"You are using: cpu\", then your laptop doesn't have GPU and you will be running training on CPU.\n",
    "\n",
    "Apple's GPU is called <b>mps</b>, and Windows or other providers' GPU is called <b>cuda</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e057b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# We will then look at the training data distribution for our own knowledge. As you remember, we have a lot more general words than either hard or soft NER. Examining our data is always important before doing any calculation so that we have an idea of what type of data and what type of data distributon/properties we are working with.\n",
    "\n",
    "# TODO Add JSON file path to your combined story training data created from \"trainingDataBuilder.py\"\n",
    "trainingDataJSONFilePath = \"/Users/Jerry/Desktop/AsteXT/AsteXTCode/AsteXTCode2025-6/training.json\"\n",
    "\n",
    "with open(trainingDataJSONFilePath, \"r\", encoding=\"utf-8\") as trainingFile:\n",
    "    trainingDataDict = json.load(trainingFile)\n",
    "\n",
    "storeCountOfDataLabels = {} # Dictionary to count how many labeles we have\n",
    "\n",
    "trainingDataSentences = trainingDataDict[\"sentences\"]\n",
    "\n",
    "for sentenceGroup in trainingDataSentences:\n",
    "    for oneLabel in sentenceGroup[\"labels\"]:\n",
    "        if oneLabel not in storeCountOfDataLabels:\n",
    "            storeCountOfDataLabels[oneLabel] = 0\n",
    "        storeCountOfDataLabels[oneLabel] += 1\n",
    "\n",
    "for label, count in sorted(storeCountOfDataLabels.items()):\n",
    "    print(f\"{label}: {count} data point(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b2269",
   "metadata": {},
   "source": [
    "You should be seeing, printed out above, that we have a lot more \"O\" (general word) than any other labels. This means that we have a <b>data imbalance</b>. This is expected, since, in language, we have a lot more generic words than named entities, hard or soft. We need to mitigate this issue when fine-tuning our model so that we can give more weight to the soft NER that we've identified.\n",
    "\n",
    "Traditionally, there are two ways to resolve data imbalance: resampling and reweighing. Resampling refers to increase or decrease the amount of training data so that we achieve a balanced proportion. Reweighting means to tell the model to give proportional weight to each training data point during training process. We will use a combination of both; specifically, we will use upsampling (increasing the number of underrepresented data points). There is also downsampling, which is underrepresenting abundant data. We will then use reweighing.\n",
    "\n",
    "The next task is to create a few functions to turn our data into machine-readable labels, i.e.: 1 for positive (soft NER) and 0 for negative (hard NER or general words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: These are two functions that we will later call on to add numerical IDs to our raw data\n",
    "\n",
    "# Create label mapping\n",
    "LabelMap = {\n",
    "    \"O\": 0,  # General words\n",
    "    \"Hard\": 0,  # Treating all hard NER as negative\n",
    "    \"Soft-Communal/Public\": 1,\n",
    "    \"Soft-Extraterrestrial/Figurative\": 2,\n",
    "    \"Soft-Institutional\": 3,\n",
    "    \"Soft-Natural\": 4,\n",
    "    \"Soft-Private\": 5,\n",
    "}\n",
    "\n",
    "def labelToID(labelStr):\n",
    "    '''\n",
    "    Turning word labels into numerical IDs'''\n",
    "    labelStr = str(labelStr).strip()\n",
    "    \n",
    "    # Check for exact matches first\n",
    "    if labelStr in LabelMap:\n",
    "        return LabelMap[labelStr]\n",
    "    \n",
    "    # Check if it's a Soft entity (handles variations)\n",
    "    if labelStr.startswith(\"Soft\"):\n",
    "        if \"Communal\" in labelStr or \"Public\" in labelStr:\n",
    "            return LabelMap[\"Soft-Communal/Public\"]\n",
    "        elif \"Extraterrestrial\" in labelStr or \"Figurative\" in labelStr:\n",
    "            return LabelMap[\"Soft-Extraterrestrial/Figurative\"]\n",
    "        elif \"Institutional\" in labelStr:\n",
    "            return LabelMap[\"Soft-Institutional\"]\n",
    "        elif \"Natural\" in labelStr:\n",
    "            return LabelMap[\"Soft-Natural\"]\n",
    "        elif \"Private\" in labelStr:\n",
    "            return LabelMap[\"Soft-Private\"]\n",
    "    \n",
    "    # Check if it's Hard NER\n",
    "    if labelStr.startswith(\"Hard\"):\n",
    "        return 0 \n",
    "    \n",
    "    if labelStr in {\"O\"}: # General word\n",
    "        return 0\n",
    "    \n",
    "    # Edge case safety: if there is an unknown label, we treat it as 0. However, if you followed \"trainingDataBuilder.py\", this scenario should probably not happen\n",
    "    print(f\"Warning: Unknown label '{labelStr}', treating as O\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: These functions help us label our datasets in a machine-readable manner\n",
    "\n",
    "# See if a word is soft NER\n",
    "def isSoft(label):\n",
    "    return str(label).startswith(\"Soft\")\n",
    "\n",
    "# Or if it is hard NER\n",
    "def isHard(label):\n",
    "    return str(label).startswith(\"Hard\")\n",
    "\n",
    "def makeTokenizeAndAlignFn(tokenizer, labelAllSubtokens=False):\n",
    "    '''This function uses the tokenizer to turn our labels into tokenized numerical values for each word.\n",
    "\n",
    "    Default parameters:\n",
    "    \"labelAllSubtokens\" refers to, if a work is broken down into multiple subtokens (e.g. \"institutionalization\" tokenized into two tokens: \"institution\" and \"alization\", whether we should give a label to both tokens or just the first one). This is set to False as default.\n",
    "    '''\n",
    "    def tokenizeAndAlignLabels(batch):\n",
    "        tokenizedInputs = tokenizer(\n",
    "            batch[\"tokens\"],\n",
    "            is_split_into_words=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        allLabels = []\n",
    "        for i, labelsStr in enumerate(batch[\"labels\"]):\n",
    "            wordIds = tokenizedInputs.word_ids(batch_index=i)\n",
    "            previousWordId = None\n",
    "            labelIds = []\n",
    "            \n",
    "            for wordId in wordIds:\n",
    "                if wordId is None:\n",
    "                    # Special tokens get -100 (ignored in loss)\n",
    "                    labelIds.append(-100)\n",
    "                elif wordId != previousWordId:\n",
    "                    # if a word is broken into subtokens, we use the first subtoken\n",
    "                    label_id = labelToID(labelsStr[wordId])\n",
    "                    labelIds.append(label_id)\n",
    "                else:\n",
    "                    # Continuation subword\n",
    "                    if labelAllSubtokens:\n",
    "                        label_id = labelToID(labelsStr[wordId])\n",
    "                        labelIds.append(label_id)\n",
    "                    else:\n",
    "                        labelIds.append(-100)  # Ignore subwords\n",
    "                \n",
    "                previousWordId = wordId\n",
    "            \n",
    "            allLabels.append(labelIds)\n",
    "\n",
    "        tokenizedInputs[\"labels\"] = allLabels\n",
    "        return tokenizedInputs\n",
    "\n",
    "    return tokenizeAndAlignLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0b1d0",
   "metadata": {},
   "source": [
    "Now, after creating a few helper functions that turn our data into formats recognizable by BERT, we will need to start manipulating our data so that they are not as imbalanced as what we've seen earlier on. We will first look at the ratio of data before upsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: This step oversamples our positive data points (soft NERs)\n",
    "\n",
    "def viewDataRatioBeforeBalancing(records):\n",
    "    ''' \n",
    "    Show distribution of each Soft category as raw data before resampling\n",
    "    '''\n",
    "    categoryCount = Counter()\n",
    "    totalTokens = 0\n",
    "    \n",
    "    for record in records:\n",
    "        for label in record[\"labels\"]:\n",
    "            totalTokens += 1\n",
    "            labelString = str(label)\n",
    "            \n",
    "            if labelString.startswith(\"Soft-\"):\n",
    "\n",
    "                categoryCount[labelString] += 1\n",
    "            elif labelString.startswith(\"Hard\") or labelString in {\"O\", \"0\"}:\n",
    "                categoryCount[\"Non-Soft\"] += 1\n",
    "    \n",
    "    print(\"Label distribution\")\n",
    "    print(f\"\\nNon-Soft tokens: {categoryCount['Non-Soft']}\")\n",
    "    print(f\"\\nSoft categories:\")\n",
    "    \n",
    "    totalSoftNER = 0\n",
    "    for category, count in sorted(categoryCount.items()):\n",
    "        if category.startswith(\"Soft-\"):\n",
    "            print(f\"  {category}: {count} tokens\")\n",
    "            totalSoftNER += count\n",
    "    \n",
    "    print(f\"\\nTotal Soft tokens: {totalSoftNER}\")\n",
    "    print(f\"Ratio (Non-Soft:Soft): {categoryCount['Non-Soft']/totalSoftNER:.1f}:1\")\n",
    "\n",
    "viewDataRatioBeforeBalancing(trainingDataSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b08257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to balance our data and see the ratio after balancing\n",
    "\n",
    "# TODO (at the end of this code cell): you need to input a ratio in the \"targetRatio\" parameter when doing function call at the end of this cell block. There is no right or wrong answer, as this is an architectural choice. You can train the miodel \n",
    "\n",
    "def balanceDatasetByCategory(records, targetRatio=10.0, boostRareCategories=True, randomSeed=42, maxRepeatPerSentence=30, probabilistic=True):\n",
    "    '''\n",
    "    Balanced dataset builder with protections against extreme duplication.\n",
    "    Parameters\n",
    "      - records: list of sentence dicts\n",
    "      - targetRatio: desired neg:pos token ratio\n",
    "      - boostRareCategories: keep your boosting behavior\n",
    "      - maxRepeatInSentence: hard cap for how many times a single sentence may be repeated. We want to avoid the case where we overly repeat one sentence\n",
    "      - probabilistic: if True, use sampling-with-replacement probabilities instead of literal duplication. We want to set it to True\n",
    "\n",
    "    This function returns a list of shuffled records\n",
    "    '''\n",
    "    random.seed(randomSeed)\n",
    "\n",
    "    # Categorize sentences (same as before)\n",
    "    sentenceByCategoryDict = {\n",
    "        \"Non-Soft\": [],\n",
    "        \"Soft-Communal/Public\": [],\n",
    "        \"Soft-Extraterrestrial/Figurative\": [],\n",
    "        \"Soft-Institutional\": [],\n",
    "        \"Soft-Natural\": [],\n",
    "        \"Soft-Private\": [],\n",
    "        \"Multiple-Soft\": []\n",
    "    }\n",
    "\n",
    "    for record in records:\n",
    "        softCategoriesInRecord = set()\n",
    "        for label in record[\"labels\"]:\n",
    "            s = str(label)\n",
    "            if s.startswith(\"Soft-\"):\n",
    "                softCategoriesInRecord.add(s)\n",
    "        if not softCategoriesInRecord:\n",
    "            sentenceByCategoryDict[\"Non-Soft\"].append(record)\n",
    "        elif len(softCategoriesInRecord) > 1:\n",
    "            sentenceByCategoryDict[\"Multiple-Soft\"].append(record)\n",
    "        else:\n",
    "            category = list(softCategoriesInRecord)[0]\n",
    "            sentenceByCategoryDict[category].append(record)\n",
    "\n",
    "    # Count tokens (same as before)\n",
    "    allSoftNERSentences = []\n",
    "    for cat, sents in sentenceByCategoryDict.items():\n",
    "        if cat != \"Non-Soft\":\n",
    "            allSoftNERSentences.extend(sents)\n",
    "\n",
    "    softNERToken = sum(\n",
    "        sum(1 for lbl in r[\"labels\"] if str(lbl).startswith(\"Soft-\"))\n",
    "        for r in allSoftNERSentences\n",
    "    )\n",
    "    negativeDataToken = sum(\n",
    "        sum(1 for lbl in r[\"labels\"] if not str(lbl).startswith(\"Soft-\"))\n",
    "        for r in records\n",
    "    )\n",
    "\n",
    "    currentRatioOfData = negativeDataToken / softNERToken if softNERToken > 0 else float(\"inf\")\n",
    "    dataRepeatFactor = max(1, int(currentRatioOfData / targetRatio))\n",
    "\n",
    "    # Build balanced_records\n",
    "    balancedDataRecords = list(sentenceByCategoryDict[\"Non-Soft\"])  # start with all negatives\n",
    "\n",
    "    if not probabilistic:\n",
    "        # literal duplication but with caps\n",
    "        if boostRareCategories:\n",
    "            # compute category sizes\n",
    "            categorySize = {\n",
    "                cat: len(sentences)\n",
    "                for cat, sentences in sentenceByCategoryDict.items()\n",
    "                if cat != \"Non-Soft\" and sentences\n",
    "            }\n",
    "            maxDataSize = max(categorySize.values()) if categorySize else 1\n",
    "\n",
    "            for category, sentences in sentenceByCategoryDict.items():\n",
    "                if category == \"Non-Soft\" or not sentences:\n",
    "                    continue\n",
    "                size = len(sentences)\n",
    "                dataBoostFactor = maxDataSize / size if size > 0 else 1.0\n",
    "                repeatDataFactor = int(dataRepeatFactor * dataBoostFactor)\n",
    "                # enforce per-sentence cap\n",
    "                repeatDataFactor = min(repeatDataFactor, maxRepeatPerSentence)\n",
    "\n",
    "                # replicate sentences but ensure not more than cap per individual sentence\n",
    "                for s in sentences:\n",
    "                    balancedDataRecords.extend([s] * repeatDataFactor)\n",
    "        else:\n",
    "            # uniform oversampling, but cap repeats per sentence\n",
    "            repeatDataFactor = min(dataRepeatFactor, maxRepeatPerSentence)\n",
    "            for s in allSoftNERSentences:\n",
    "                balancedDataRecords.extend([s] * repeatDataFactor)\n",
    "    else:\n",
    "        # probabilistic upsampling: sample with replacement from soft sentences\n",
    "        # compute per-category sampling probabilities proportional to desired boosts\n",
    "        print(\"Probabilistic upsampling mode: sampling with replacement rather than full duplication.\")\n",
    "        desiredTotalSoftData = int(negativeDataToken / targetRatio) if targetRatio > 0 else len(allSoftNERSentences)\n",
    "        if desiredTotalSoftData <= 0:\n",
    "            desiredTotalSoftData = len(allSoftNERSentences)\n",
    "\n",
    "        # Build a flat list of (sentence, category) for sampling\n",
    "        flat = []\n",
    "        for cat, sents in sentenceByCategoryDict.items():\n",
    "            if cat == \"Non-Soft\" or not sents:\n",
    "                continue\n",
    "            # per-sentence weight: boost small categories by inverse size (like before)\n",
    "            cat_size = len(sents)\n",
    "            for s in sents:\n",
    "                # smaller categories get slightly higher sampling weight\n",
    "                w = 1.0 * (max(1, max(len(sentenceByCategoryDict[c]) for c in sentenceByCategoryDict if c != \"Non-Soft\")) / max(1, cat_size))\n",
    "                flat.append((s, w))\n",
    "\n",
    "        # normalize weights and sample\n",
    "        sentenceList, weights = zip(*flat)\n",
    "        weights = [float(w) for w in weights]\n",
    "        totalWeights = sum(weights)\n",
    "        probs = [w / totalWeights for w in weights]\n",
    "\n",
    "        # Draw desiredTotalSoftData samples with replacement, but cap per individual sentence\n",
    "        from collections import defaultdict\n",
    "        chosen_counts = defaultdict(int)\n",
    "        for _ in range(desiredTotalSoftData):\n",
    "            idx = random.choices(range(len(sentenceList)), probs)[0]\n",
    "            sent = sentenceList[idx]\n",
    "            if chosen_counts[id(sent)] >= maxRepeatPerSentence:\n",
    "                continue  # skip adding more of this particular instance\n",
    "            balancedDataRecords.append(sent)\n",
    "            chosen_counts[id(sent)] += 1\n",
    "\n",
    "    random.shuffle(balancedDataRecords)\n",
    "\n",
    "    # final stat\n",
    "    finalSoftNER = sum(\n",
    "        sum(1 for lbl in r[\"labels\"] if str(lbl).startswith(\"Soft-\"))\n",
    "        for r in balancedDataRecords\n",
    "    )\n",
    "    finalNegativeData = sum(\n",
    "        sum(1 for lbl in r[\"labels\"] if not str(lbl).startswith(\"Soft-\"))\n",
    "        for r in balancedDataRecords\n",
    "    )\n",
    "\n",
    "    print(\"\\nOVERSAMPLING RESULTS:\")\n",
    "    print(f\"  total sentences after balance: {len(balancedDataRecords)}\")\n",
    "    print(f\"  Soft tokens: {finalSoftNER}, Non-Soft tokens: {finalNegativeData}\")\n",
    "    if finalSoftNER:\n",
    "        print(f\"  Final ratio (Non-Soft:Soft): {finalNegativeData/finalSoftNER:.1f}:1\")\n",
    "    else:\n",
    "        print(\"Warning: no Soft tokens in balanced records\")\n",
    "\n",
    "    return balancedDataRecords\n",
    "\n",
    "# TODO: determine a ratio of positive:negative data. Note that it won't necessarily result in exactly the ratio that you determine. For example, you might put in a 1:5 ratio but the function might still provide a 1:10, but it will still be a great improvement from the, say 1:50 ratio from the raw data\n",
    "balancedRecords = balanceDatasetByCategory(trainingDataSentences, targetRatio=5.0, maxRepeatPerSentence=30, probabilistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca920f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying\n",
    "\n",
    "# Calculate class weights and reweigh them as a way to further mitigate data imbalanace.\n",
    "# Reweighing means giving more weight to data points that are less represented\n",
    "\n",
    "def calculateClassWeights(records, minCount=1, clipRatio=10.0): # clipRatio prevents providing an overly large number when reweighing data \n",
    "    counts = Counter()\n",
    "    for r in records:\n",
    "        for lbl in r[\"labels\"]:\n",
    "            counts[labelToID(lbl)] += 1\n",
    "    total = sum(counts.values())\n",
    "    numOfCategories = 6\n",
    "    weights = []\n",
    "    for cid in range(numOfCategories):\n",
    "        cnt = max(counts.get(cid, 0), minCount)\n",
    "        w = total / (numOfCategories * cnt)\n",
    "        w = min(w, clipRatio)\n",
    "        weights.append(w)\n",
    "    return torch.tensor(weights, dtype=torch.float32)  # keep on CPU and get transferred over to GPU when needed\n",
    "\n",
    "classWeights = calculateClassWeights(balancedRecords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690aae1f",
   "metadata": {},
   "source": [
    "Now, we will split our training data into two: \n",
    "- training data (sorry for naming it the same way)\n",
    "- validation data\n",
    "\n",
    "Validation data is different from testing data is that validation data is also labeled, but we won't be usingg validation data in model training (otherwise the model would be \"cheating\")\n",
    "\n",
    "The function below, `def makeValidationSplitByCategoryseparates(...):`, will split our data into training and validation according to the ratio set in the parameter \"valFrac\". If \"valFrac==0.05\", it means that 5% of our labeled data will be set aside to be validation data. You can change this based on how many data you've labeled.\n",
    "\n",
    "The good thing about the \"makeValidationSplitByCategoryseparates\" function is that it takes data imbalance into consideration when selecting data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO at the end of this code cell: this function splits our labeled data into training and validation sets  \n",
    "def makeValidationSplitByCategory(records, validationProportion=0.05, randomSeed=42, minHoldoutPerCategory=1):\n",
    "    '''Create training and validation splits and ensure at least one sentence per Soft category\n",
    "    when possible, and not removing the only instance from train.'''\n",
    "\n",
    "    random.seed(randomSeed)\n",
    "    # group sentences by the single soft category present, or 'Non-Soft' or 'Multiple-Soft'\n",
    "    groups = defaultdict(list)\n",
    "    for i, r in enumerate(records):\n",
    "        softCats = {str(lbl) for lbl in r[\"labels\"] if str(lbl).startswith(\"Soft-\")}\n",
    "        if not softCats:\n",
    "            groups[\"Non-Soft\"].append(i)\n",
    "        elif len(softCats) > 1:\n",
    "            groups[\"Multiple-Soft\"].append(i)\n",
    "        else:\n",
    "            cat = list(softCats)[0]\n",
    "            groups[cat].append(i)\n",
    "\n",
    "    # Build initial validation indices by reserving 1 sentence for categories with 2 or more sentences\n",
    "    valIndices = set()\n",
    "    for cat, idxs in groups.items():\n",
    "        if cat == \"Non-Soft\":\n",
    "            continue\n",
    "        if len(idxs) >= (minHoldoutPerCategory + 1):\n",
    "            # choose up to minHoldoutPerCategory examples to hold out\n",
    "            chosen = random.sample(idxs, min(minHoldoutPerCategory, len(idxs)))\n",
    "            valIndices.update(chosen)\n",
    "\n",
    "    # Now, ensure the validation part of the data is met by sampling randomly from remaining indices\n",
    "    allIndices = set(range(len(records)))\n",
    "    remaining = list(allIndices - valIndices)\n",
    "    targetValSize = max(int(len(records) * validationProportion), len(valIndices))\n",
    "    extraNeeded = max(0, targetValSize - len(valIndices))\n",
    "    if extraNeeded > 0:\n",
    "        extra = random.sample(remaining, min(extraNeeded, len(remaining)))\n",
    "        valIndices.update(extra)\n",
    "\n",
    "    trainIndices = sorted(list(allIndices - valIndices))\n",
    "    valIndices = sorted(list(valIndices))\n",
    "\n",
    "    print(f\"Total records: {len(records)}: train: {len(trainIndices)}, validation: {len(valIndices)}\")\n",
    "\n",
    "    # show per-category counts\n",
    "    def catCounts(idxList):\n",
    "        c = Counter()\n",
    "        for i in idxList:\n",
    "            r = records[i]\n",
    "            for lbl in r[\"labels\"]:\n",
    "                s = str(lbl)\n",
    "                if s.startswith(\"Soft-\"):\n",
    "                    c[s] += 1\n",
    "                elif s.startswith(\"Hard\") or s in {\"O\", \"0\"}:\n",
    "                    c[\"Non-Soft\"] += 1\n",
    "        return c\n",
    "\n",
    "    print(\"Train token counts by category (sampled):\", {k: v for k, v in catCounts(trainIndices).items()})\n",
    "    print(\"Validation token counts by category (sampled):\", {k: v for k, v in catCounts(valIndices).items()})\n",
    "\n",
    "    trainRecords = [records[i] for i in trainIndices]\n",
    "    validationRecords = [records[i] for i in valIndices]\n",
    "\n",
    "    return trainRecords, validationRecords\n",
    "\n",
    "# TODO: decide on how much of your labeled data you want to set aside for data validation. I put down 5% (or 0.05). You can start there and adjust as you see fit.\n",
    "validationSetProportion = 0.05  # This means we are using 5% of data for validation. you can change this. But judging from the size fo each of our labeled data, I won't recommend using more tahn 10% for validation\n",
    "trainingRecords, validationRecords = makeValidationSplitByCategory(balancedRecords, validationProportion=validationSetProportion, randomSeed=42)\n",
    "# remember the variable \"validationRecords\". We will use it after a while, after we've trained the model, to do validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: tokenize our training data or training\n",
    "\n",
    "def prepareCategorySoftNerTrainOnly(records, modelName=\"bert-base-uncased\", labelAllSubtokens=False, returnDatasetDict=False):\n",
    "    '''\n",
    "    model name is set to bert-base-uncased as default, since we are using this in our tutorial\n",
    "    '''\n",
    "    # Records: list of dicts with keys \"tokens\" and \"labels\" (as created from \"trainingDataBuilder.py\")\n",
    "    for r in records:\n",
    "        assert len(r[\"tokens\"]) == len(r[\"labels\"]), \"number of tokens and number of labels must match\" #this will throw an error if your training data JSON somehow has more words than labels. Should not happen if you followed the trainingDataBuilder.py\n",
    "\n",
    "    ds = Dataset.from_list(records)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName, use_fast=True)\n",
    "\n",
    "    fn = makeTokenizeAndAlignFn(tokenizer, labelAllSubtokens=labelAllSubtokens) # call tokenizer wrapper function\n",
    "    tokenizedTrain = ds.map(fn, batched=True)\n",
    "\n",
    "    if returnDatasetDict:\n",
    "        return DatasetDict(train=tokenizedTrain)\n",
    "    return tokenizedTrain\n",
    "\n",
    "tokenizedTrainingData = prepareCategorySoftNerTrainOnly(trainingRecords) # Note here that we are only using training data, not validation data, from the previous code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: double-checking whether we are using GPU or CPU\n",
    "\n",
    "# device is a torch.device, e.g. torch.device(\"mps\") or torch.device(\"cuda\")\n",
    "print(\"device:\", device, \"device.type:\", device.type)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    batchSize = 32\n",
    "elif device.type == \"mps\":\n",
    "    batchSize = 16\n",
    "else:\n",
    "    batchSize = 8\n",
    "\n",
    "print(f\"Using batch size: {batchSize} because you are using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying\n",
    "\n",
    "# Here, we are loading BERT\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=6, # because we are creating 6 categories (5 different soft NER and 1 negative data)\n",
    "    label2id=LabelMap\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dataCollator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: to make sure that we have three columns:['labels', 'input_ids', 'attention_mask']\n",
    "# After you run this code, you should see something like this:\n",
    "'''Columns: ['labels', 'input_ids', 'attention_mask']\n",
    "{'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, -100, -100], 'input_ids': [101, 2021, 3904, 1997, 2068, 2064, 4339, 1999, 2256, 3320, 4155, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "Remaining columns: ['labels', 'input_ids', 'attention_mask']\n",
    "'''\n",
    "#The numbers might not be exactly the same, but make sure that the last line being printed out is: Remaining columns: ['labels', 'input_ids', 'attention_mask']\n",
    "\n",
    "print(\"Columns:\", tokenizedTrainingData.column_names)\n",
    "print(tokenizedTrainingData[0])\n",
    "\n",
    "# Remove non-tensor columns, such as our charSpans label that shows where in a sentence a word is at.\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "cols_to_remove = [c for c in tokenizedTrainingData.column_names if c not in cols_to_keep]\n",
    "tokenizedTrainingData = tokenizedTrainingData.remove_columns(cols_to_remove)\n",
    "print(\"Remaining columns:\", tokenizedTrainingData.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed059677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying\n",
    "# test before training, get one batch from DataLoader\n",
    "\n",
    "trainingDataloader = DataLoader(\n",
    "    tokenizedTrainingData,\n",
    "    batch_size=batchSize,\n",
    "    collate_fn=dataCollator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "batch = next(iter(trainingDataloader))\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "print(\"Batched keys:\", batch.keys())\n",
    "# when printing batch keys, you should see something like \"dict_keys(['input_ids', 'attention_mask', 'labels'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cfd95",
   "metadata": {},
   "source": [
    "Now, we've finished preparing our data for training. Model training happens in multiple cycles, or some might call them \"steps.\" The number of steps that we want to use in training is an architectural design on our side. Each step makes the model \"understand\" our data better, but we don't want to iterate too many steps, since that would cause overfitting (a scenaraio where the model learns the training data well but does not generalize well). This means that a good practice is to try training a model multiple times, using different number of steps and see which one provides the best result.\n",
    "\n",
    "Before starting the full training cycle, we will experiment with just one step and see if our code works. The code cell below might take a while to execute (between about 20 seconds to a few minutes, depending on whether you are using GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb08bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying\n",
    "\n",
    "# This code cell replicates one step of training\n",
    "dl = DataLoader(tokenizedTrainingData, batch_size=batchSize, collate_fn=dataCollator)\n",
    "batch = next(iter(dl))\n",
    "batch = {k: v.to(device) for k,v in batch.items()}\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "optimizer.zero_grad()\n",
    "outputs = model(**batch)\n",
    "loss = outputs.loss if hasattr(outputs,\"loss\") else torch.nn.functional.cross_entropy(\n",
    "    outputs.logits.view(-1,6), batch[\"labels\"].view(-1), ignore_index=-100)\n",
    "print(\"Initial loss: \", loss.item())\n",
    "loss.backward()\n",
    "\n",
    "print(\"Any empty or none gradients?\", any(torch.isnan(p.grad).any().item() for p in model.parameters() if p.grad is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d2f03",
   "metadata": {},
   "source": [
    "Intuitively, \"loss\" refers to the size of error between the model's predicted points and the true points. At this point, we just want to see if the model, after one round of training, will generate for us a loss value or not, regardless of how large the number is. If you see that the initial loss number is something unreasonable, such as 0.00 (your model will make errors on the first training step) or \"NaN\", then your code previously might have some issues.\n",
    "\n",
    "You should also see a \"False\" for \"Any empty or none gradients?\" This means that all gradients (the vector representing the direction that the model should move next in training) are operating correctly. Later on, during the main training steps, we will keep a close eye on whether all gradients are operating correctly at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff2d19",
   "metadata": {},
   "source": [
    "The below code is the main training loop. \n",
    "\n",
    "Since each of us is using different data, we will use different training hyperparameters. There are many hyperparameters to use. The two that we will focus the most on are \"trainingSteps\" and \"learningRate\".\n",
    "\n",
    "I've set a default value for your training steps and learning rate here, as 400 and 1e-6, respectively. You are to change them according to the training result.\n",
    "\n",
    "Some suggested range of training steps: `200-1200`\n",
    "Some suggested choices of learning rate: `3e-5, 1e-5, 5e-6, 3e-6, 1e-6, 5e-7`\n",
    "\n",
    "Training steps, as mentioned before, represents how manys steps will the model use in training\n",
    "Learning rate refers to how precise of a \"step\" the model take after each iteration. In other words, how much the model learns at each step. A large learning rate means that the model learns quickly, saving training time but also risking losing out on minute details in the training data. A smaller learning rate meeans that the model is more meticulous but means training will take longer.\n",
    "\n",
    "There are many other hyperparameters that one can use in machine learning. We will just focus on using these two for AsteXT. You are more than welcome to learn about others.\n",
    "\n",
    "---\n",
    "F1 score (ranging between 0 and 1) is a measurement of how well a model is performing on a classification task. (F1 is the combination of precision and recall scores). Since we do not have a robust validation data, the F1 score will be slightly arbitrary and will very likely overestimate the actual F1 score, but at least it's better than not having one. \n",
    "\n",
    "When you run the training code cell below, you will see lines like this printed out:\n",
    "`Step 1/400  loss=1.8802  F1=0.048  any_nan_grad=False  any_nan_param=False`\n",
    "\n",
    "You will get an update like this every 10 steps (or however many steps as you determine in the variable \"printEvery\"). \"loss=\" refers to the loss value, \"F1=\" refers to the rough F1 score at that step, and \"any_nan_grad=\" and \"any_nan_param=\" refers to whether, at that stage, any parameters or gradients are invalid. If False, then it means no parameter or gradient is invalid. We should try to keep it that way. If at a training step where one or both of them become \"True,\" then you can take note of how many steps it took the model to reach that point and try decreasing the \"trainingSteps\" hyperparameter to that number.\n",
    "\n",
    "In your training:\n",
    "- <b>Aim for a rough F1 score of at least `0.65`. The more the better.</b>\n",
    "- <b>Aim for a loss value of around `0.4`. The lower the better.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyperparameters: try out multiple options for training steps and learning rate. \n",
    "# Suggested range of training steps: 200-1200\n",
    "# Suggested choices of learning rate: 3e-5, 1e-5, 5e-6, 3e-6, 1e-6, 5e-7\n",
    "\n",
    "trainingSteps = 400\n",
    "learningRate = 1e-6\n",
    "# --------\n",
    "\n",
    "printEvery = 10 # This refers to how often the function prints something below to indicate training progress. You can leave it at 10 as default.\n",
    "\n",
    "trainingDataLoader = DataLoader(tokenizedTrainingData, batch_size=batchSize, shuffle=True, collate_fn=dataCollator)\n",
    "it = iter(trainingDataLoader)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# To double-check again whether you are using GPU or CPU\n",
    "print(\"Model device:\", next(model.parameters()).device) \n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learningRate)\n",
    "\n",
    "for step in range(1, trainingSteps + 1):\n",
    "    try:\n",
    "        batch = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(trainingDataLoader)\n",
    "        batch = next(it)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    logits = outputs.logits.float()\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    weight = classWeights.to(logits.device).to(logits.dtype)\n",
    "    crossEntropyLossFunc = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=-100) # using cross entropy loss\n",
    "    loss = crossEntropyLossFunc(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute quick F1 score\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    mask = labels != -100\n",
    "    yTrue = labels[mask].cpu().numpy()\n",
    "    yPred = preds[mask].cpu().numpy()\n",
    "    # F1 across all classes (macro)\n",
    "    batchF1 = f1_score(yTrue, yPred, average='macro', zero_division=0)\n",
    "\n",
    "    anyNoneGradients = any((p.grad is not None) and torch.isnan(p.grad).any().item() for p in model.parameters())\n",
    "    anyNoneParameters = any(torch.isnan(p).any().item() for p in model.parameters())\n",
    "\n",
    "    if step % printEvery == 0 or step == 1:\n",
    "        print(f\"Step {step}/{trainingSteps}  loss={loss.item():.4f}  F1={batchF1:.3f}  any_nan_grad={anyNoneGradients}  any_nan_param={anyNoneParameters}\")\n",
    "\n",
    "    if anyNoneGradients or anyNoneParameters:\n",
    "        print(\"Blank gradients or parameters detected, stopping early.\")\n",
    "        print(f\"Stopped at step: {step}\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8212aae",
   "metadata": {},
   "source": [
    "Now, we need to store your model. The trained model will be represented as a folder after being stored locally. Please make sure to do this following code cell <b>right after</b> training the model. Do not run other code cells in between. These two steps must be done together!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5731f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add a Save current safe model (so you can revert)\n",
    "\n",
    "nameOfFineTunedModel = \"NameYourFineTunedModelHere\"\n",
    "fineTunedModelStoragePath = \"./\" + nameOfFineTunedModel\n",
    "\n",
    "model.save_pretrained(fineTunedModelStoragePath)\n",
    "tokenizer.save_pretrained(fineTunedModelStoragePath)\n",
    "print(\"Saved model to\", fineTunedModelStoragePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5396a",
   "metadata": {},
   "source": [
    "After running the above code cell, look in your directory to see if you could find a folder called the name that you gave to your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71870222",
   "metadata": {},
   "source": [
    "Now we will try to calculate our F1 score and study our model's performance on data that the model hasn't see before, through the validation data that we've set aside back in the function `makeValidationSplitByCategory`. Run this following code cell and observe the result printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: now we will try to calculate our F1 score and study our model's performance on data that the model hasn't see. Watch out for printouts below this code cell\n",
    "\n",
    "IDToLabelDict = {\n",
    "    0: \"O\",\n",
    "    1: \"Soft-Communal/Public\",\n",
    "    2: \"Soft-Extraterrestrial/Figurative\",\n",
    "    3: \"Soft-Institutional\",\n",
    "    4: \"Soft-Natural\",\n",
    "    5: \"Soft-Private\",\n",
    "}\n",
    "\n",
    "# Tokenize / align labels for both splits using your existing function\n",
    "# Use the same tokenizer and labelAllSubtokens flag you used for training\n",
    "fn = makeTokenizeAndAlignFn(tokenizer, labelAllSubtokens=False)  # or True, same as training\n",
    "trainingDataForPostTrainingValidation = Dataset.from_list(trainingRecords)\n",
    "validationDataForPostTrainingValidation = Dataset.from_list(validationRecords)\n",
    "\n",
    "tokenizedTrain = trainingDataForPostTrainingValidation.map(fn, batched=True)\n",
    "tokenizedVal = validationDataForPostTrainingValidation.map(fn, batched=True)\n",
    "\n",
    "# Remove any extraneous string columns if present (collator expects input_ids, attention_mask, labels)\n",
    "columnsNeeded = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "def keepColumns(ds):\n",
    "    moveColumn = [column for column in ds.column_names if column not in columnsNeeded]\n",
    "    if moveColumn:\n",
    "        return ds.remove_columns(moveColumn)\n",
    "    return ds\n",
    "\n",
    "tokenizedTrain = keepColumns(tokenizedTrain)\n",
    "tokenizedVal = keepColumns(tokenizedVal)\n",
    "\n",
    "# Set format to torch (optional, but helpful if doing manual DataLoader)\n",
    "tokenizedTrain.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "tokenizedVal.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "print(\"Tokenized train/val ready. Train size:\", len(tokenizedTrain), \"Val size:\", len(tokenizedVal))\n",
    "# keep tokenizedTrainingData variable name convention if your training pipeline expects it:\n",
    "tokenizedTrainingData = tokenizedTrain\n",
    "tokenizedValidationData = tokenizedVal\n",
    "\n",
    "\n",
    "# validation that provides a yTrue and yPredict value\n",
    "def runValidationPostTraining(model, tokenizedVal, batch_size=16):\n",
    "    dl = DataLoader(tokenizedVal, batch_size=batch_size, collate_fn=dataCollator)\n",
    "    model.eval()\n",
    "    yTruePostTraining = []\n",
    "    yPredictPostTraining = []\n",
    "    for batch in dl:\n",
    "        # move inputs to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "            logits = out.logits.detach().cpu().numpy()   # shape (B, S, C)\n",
    "        labels = batch[\"labels\"].detach().cpu().numpy() # shape (B, S)\n",
    "\n",
    "        preds_ids = np.argmax(logits, axis=2)  # (B, S)\n",
    "        # iterate tokens and collect those where label != -100\n",
    "        B, S = labels.shape\n",
    "        for i in range(B):\n",
    "            for j in range(S):\n",
    "                lab = labels[i, j]\n",
    "                if lab != -100:\n",
    "                    yTruePostTraining.append(IDToLabelDict[int(lab)])\n",
    "                    yPredictPostTraining.append(IDToLabelDict[int(preds_ids[i, j])])\n",
    "    return yTruePostTraining, yPredictPostTraining\n",
    "\n",
    "yTruePostTraining, yPredictPostTraining = runValidationPostTraining(model, tokenizedValidationData, batch_size=batchSize)\n",
    "print(classification_report(yTruePostTraining, yPredictPostTraining, labels=list(IDToLabelDict.values()), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada81c87",
   "metadata": {},
   "source": [
    "Are you satisfied with the results? If not, you can retrain the model through adjusting hyperparameters, such as training steps, learning rate, resampling ratio, and others.\n",
    "\n",
    "If you are satisfied, we can now load our trained model on our testing data (the JSON file created from \"testingDataBuilder.py\") and watch our fine-tuned model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e866a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two tasks in this code cell\n",
    "# TODO One: put the file path of your fine-tuned model in the variable \"fineTunedModelPath\"\n",
    "fineTunedModelPath = \"/Users/Jerry/Desktop/AsteXT/AsteXTCode/AsteXTCode2025-6/softNERModelFinalCategoryAware\" \n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "fineTunedTokenizer = AutoTokenizer.from_pretrained(fineTunedModelPath, use_fast=True)\n",
    "fineTunedModel = AutoModelForTokenClassification.from_pretrained(fineTunedModelPath)\n",
    "\n",
    "fineTunedModel.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "fineTunedModel.eval() \n",
    "print(\"Model loaded\")\n",
    "\n",
    "# TODO Two: add the file path of your testing data JSON. Make sure this JSON was created by \"testingDataBuilder.py\"\n",
    "testingDataJsonFilePath = \"/Users/Jerry/Desktop/AsteXT/AsteXTCode/AsteXTCode2025-6/testData1.json\"\n",
    "with open(testingDataJsonFilePath, \"r\") as testingDataFile:\n",
    "    testingDataDict = json.load(testingDataFile)\n",
    "\n",
    "listOfTestingSentences = [sentence for sentenceID, sentence in testingDataDict.items()]\n",
    "print(f\"You have loaded {len(listOfTestingSentences)} sentences to test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f809663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without modifying: this code loads your testing sentences and try to apply your trained model on these sentences to make predictions\n",
    "\n",
    "def predictSoftNERCategories(text, model, tokenizer, device=None, debug=False):\n",
    "    '''\n",
    "    The \"debug\" hyperparameter is to determine whether we should print out more information when running. It doesn't affect the actual prediction of the model.\n",
    "    '''\n",
    "    if device is None:\n",
    "        device = device\n",
    "    elif isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        # max_length=max_length,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Prepare safe CPU copies for decoding\n",
    "    offsetMap = encoding[\"offset_mapping\"][0].cpu().tolist()\n",
    "    inputIDForCPU = encoding[\"input_ids\"][0].cpu().tolist()\n",
    "\n",
    "    # Move the rest to device (but keep offsets on CPU)\n",
    "    inputs = {k: v.to(device) for k, v in encoding.items() if k != \"offset_mapping\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        \n",
    "        logitCPU = logits.detach().cpu().float()\n",
    "        if torch.isnan(logitCPU).any():\n",
    "            # replace none values with very negative values so softmax is stable\n",
    "            logitCPU = torch.nan_to_num(logitCPU, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logitCPU, dim=-1)\n",
    "        confs, preds = probs.max(dim=-1)\n",
    "\n",
    "    preds = preds[0].tolist()\n",
    "    confs = confs[0].tolist()\n",
    "\n",
    "    # Convert tokens to CPU\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputIDForCPU, skip_special_tokens=False)\n",
    "\n",
    "    results = []\n",
    "    for token, predictionID, conf, (start, end) in zip(tokens, preds, confs, offsetMap):\n",
    "        # Skip special tokens and padding via offsets (special tokens often have start==end)\n",
    "        if start == end:\n",
    "            continue\n",
    "        word = text[start:end] if end > start else token\n",
    "        labelName = IDToLabelDict.get(int(predictionID), \"Unknown\")\n",
    "\n",
    "        # ensure finite confidence\n",
    "        conf = float(conf) if (conf == conf and conf not in (float(\"inf\"), float(\"-inf\"))) else 0.0\n",
    "        results.append((word, labelName, conf))\n",
    "\n",
    "    if debug:\n",
    "        print(\"Decoded counts:\", Counter([lab for _, lab, _ in results]))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---\n",
    "# Test run your fine-tuned model:\n",
    "predictionsList = []\n",
    "for sentence in listOfTestingSentences:\n",
    "    predictions = predictSoftNERCategories(sentence, fineTunedModel, fineTunedTokenizer, device=device, debug=True)\n",
    "    predictionsList.append(predictions)\n",
    "\n",
    "print(\"\\nPredictions (non-O):\")\n",
    "for prediction in predictionsList:\n",
    "    for word, label, confidence in predictions:\n",
    "        # if label != \"O\":\n",
    "            print(f\"  '{word}': {label} (confidence: {confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f8351",
   "metadata": {},
   "source": [
    "You can look at the print out to see if you think the predictions are accurate. If not, you can train the model again using different hyperparameters or add more data to it. If you are happy with the results, then you have successfully fine-tuned a BERT model to identify soft Named Entities for Asian American short stories!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359817eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b2855",
   "metadata": {},
   "source": [
    "The below code is for debug purposes. You do not need to run it unless extremely necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run unless needed or if you are curious. \n",
    "'''This code cell is intended for you to diagnose your fine-tuned model in cases where something unexpected happens'''\n",
    "# Diagnostic: why predictions are uniform 1/6\n",
    "import os, torch, numpy as np\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# === Set paths / objects you used ===\n",
    "# Update the model path to the path of your fine-tuned model:\n",
    "model_path = \"/Users/Jerry/Desktop/AsteXT/AsteXTCode/AsteXTCode2025-6/softNERModelCategoryVary_safe_checkpoint_pre200_0.4\"  \n",
    "print(\"MODEL PATH:\", model_path)\n",
    "print(\"Files in model dir:\", sorted(os.listdir(model_path)))\n",
    "\n",
    "# === Load model/tokenizer (no device move yet) ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "print(\"model.config.num_labels:\", model.config.num_labels)\n",
    "\n",
    "# === 1) PARAM checks: NaNs / zeros / sums for a few representative params ===\n",
    "def stats_for_params(m, max_show=12):\n",
    "    total = 0\n",
    "    any_nan = False\n",
    "    zero_count = 0\n",
    "    sample = []\n",
    "    for i,(n,p) in enumerate(m.named_parameters()):\n",
    "        total += p.numel()\n",
    "        cpu = p.detach().cpu()\n",
    "        nan = bool(torch.isnan(cpu).any())\n",
    "        any_nan = any_nan or nan\n",
    "        zeros = int((cpu == 0).sum().item())\n",
    "        zero_count += zeros\n",
    "        if i < max_show:\n",
    "            sample.append((n, tuple(cpu.shape), float(cpu.abs().sum().item()), nan, zeros))\n",
    "    return {\"total\": total, \"any_nan\": any_nan, \"zero_count\": zero_count, \"sample\": sample}\n",
    "\n",
    "stats = stats_for_params(model)\n",
    "print(\"\\n=== PARAM STATS SUMMARY ===\")\n",
    "print(\" total params:\", stats[\"total\"])\n",
    "print(\" any NaN in params?:\", stats[\"any_nan\"])\n",
    "print(\" total zeros:\", stats[\"zero_count\"])\n",
    "print(\" sample params (name,shape,sum_abs,hasNaN,zero_count):\")\n",
    "for r in stats[\"sample\"]:\n",
    "    print(\" \", r)\n",
    "\n",
    "# === 2) Classifier layer explicit check ===\n",
    "print(\"\\n=== CLASSIFIER PARAMS ===\")\n",
    "for n,p in model.named_parameters():\n",
    "    if any(key in n.lower() for key in (\"classifier\",\"score\",\"out\",\"proj\")):\n",
    "        cpu = p.detach().cpu()\n",
    "        print(n, p.shape, \"sum_abs=\", float(cpu.abs().sum().item()), \"any_nan=\", bool(torch.isnan(cpu).any()), \"zeros=\", int((cpu==0).sum().item()))\n",
    "\n",
    "# === 3) If you still have balancedRecords in memory, print class token counts; otherwise, recompute counts from tokenized dataset.\n",
    "try:\n",
    "    balancedRecords  # noqa\n",
    "    have_balanced = True\n",
    "except NameError:\n",
    "    have_balanced = False\n",
    "\n",
    "print(\"\\n=== DATA / LABEL DISTRIBUTION ===\")\n",
    "if have_balanced:\n",
    "    print(\"Using balancedRecords variable (found in current session).\")\n",
    "    counts = Counter()\n",
    "    total_tokens = 0\n",
    "    for r in balancedRecords:\n",
    "        for lbl in r[\"labels\"]:\n",
    "            total_tokens += 1\n",
    "            counts[labelToID(lbl)] += 1\n",
    "    for cid,cnt in sorted(counts.items()):\n",
    "        print(f\"  label {cid} ({IDToLabelDict.get(cid,'?')}): {cnt} tokens\")\n",
    "    print(\" total tokens:\", total_tokens)\n",
    "else:\n",
    "    # try to infer from tokenizedTrainingData if present\n",
    "    try:\n",
    "        tokenizedTrainingData  # noqa\n",
    "        print(\"Using tokenizedTrainingData variable (found in current session).\")\n",
    "        # Inspect first few examples to ensure labels present and are ints\n",
    "        print(\"Columns:\", tokenizedTrainingData.column_names)\n",
    "        for i in range(min(3, len(tokenizedTrainingData))):\n",
    "            ex = tokenizedTrainingData[i]\n",
    "            print(f\"\\nExample {i} keys and types:\")\n",
    "            for k,v in ex.items():\n",
    "                t = type(v)\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    sample = v[:10]\n",
    "                    print(f\"  {k}: type={t}, len={len(v)}, sample={sample}\")\n",
    "                else:\n",
    "                    print(f\"  {k}: type={t}, sample={str(v)[:80]}\")\n",
    "        # aggregate label ids\n",
    "        all_label_ids = []\n",
    "        for i in range(min(2000, len(tokenizedTrainingData))):\n",
    "            lab = tokenizedTrainingData[i][\"labels\"]\n",
    "            # some labels are lists of ints with -100  flatten and count non -100\n",
    "            if isinstance(lab, list):\n",
    "                all_label_ids.extend([x for x in lab if x != -100])\n",
    "        print(\"Found label id distribution (sampled up to 2000 examples):\", Counter(all_label_ids))\n",
    "    except NameError:\n",
    "        print(\"No balancedRecords or tokenizedTrainingData found in this session. Please run diagnostics with those available.\")\n",
    "\n",
    "# === 4) Inspect classWeights if in memory ===\n",
    "try:\n",
    "    classWeights  # noqa\n",
    "    print(\"\\nclassWeights (tensor):\", classWeights)\n",
    "    print(\" classWeights dtype/device:\", classWeights.dtype, classWeights.device)\n",
    "    print(\" classWeights values:\", classWeights.tolist())\n",
    "except NameError:\n",
    "    print(\"\\nclassWeights variable not found in this session.\")\n",
    "\n",
    "# === 5) Run a forward on a sample sentence and print logits (no softmax) and their stats ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if getattr(torch.backends,'mps',None) is not None and torch.backends.mps.is_available()\n",
    "                      else \"cpu\")\n",
    "print(\"\\nRunning single-sample forward on device:\", device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "text = \"The Academy again assures me they are searching for fresh engineers.\"\n",
    "enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128, return_offsets_mapping=True)\n",
    "offsets = enc[\"offset_mapping\"][0].cpu().tolist()\n",
    "input_ids_cpu = enc[\"input_ids\"][0].cpu().tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_cpu, skip_special_tokens=False)\n",
    "\n",
    "inputs = {k: v.to(device) for k,v in enc.items() if k != \"offset_mapping\"}\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "    logits = out.logits.detach().cpu().float()  # move to cpu float\n",
    "\n",
    "print(\"LOGITS shape:\", logits.shape)\n",
    "print(\"Tokens+offsets:\", [(i, tokens[i], offsets[i]) for i in range(min(8, len(tokens)))])\n",
    "print(\"\\nLogits first 8 positions:\\n\", logits[0,:8,:].numpy())\n",
    "print(\"Per-position min max (first 8):\")\n",
    "arr = logits[0].numpy()\n",
    "print(\" mins:\", np.min(arr[:8,:], axis=1))\n",
    "print(\" maxs:\", np.max(arr[:8,:], axis=1))\n",
    "print(\"Any NaNs in logits?:\", np.isnan(arr).any())\n",
    "print(\"Unique rows (rounded):\", np.unique(np.round(arr,6), axis=0).shape)\n",
    "\n",
    "# === 6) Compare classifier params to base model (are they unchanged?) ===\n",
    "print(\"\\nComparing classifier weights to fresh base model (bert-base-uncased)...\")\n",
    "base = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=model.config.num_labels)\n",
    "def collect_classifier(m):\n",
    "    d={}\n",
    "    for n,p in m.named_parameters():\n",
    "        if any(k in n.lower() for k in (\"classifier\",\"score\",\"out\",\"proj\")) or n.endswith(\"classifier.weight\") or n.endswith(\"classifier.bias\"):\n",
    "            d[n]=p.detach().cpu().numpy()\n",
    "    return d\n",
    "base_cls = collect_classifier(base)\n",
    "saved_cls = collect_classifier(model)\n",
    "print(\"Classifier param names in base:\", list(base_cls.keys()))\n",
    "print(\"Classifier param names in saved:\", list(saved_cls.keys()))\n",
    "for k in base_cls:\n",
    "    if k in saved_cls:\n",
    "        diff = np.linalg.norm(saved_cls[k].ravel() - base_cls[k].ravel())\n",
    "        print(\" param:\", k, \"L2 diff from base:\", diff, \" sum_abs_saved:\", np.abs(saved_cls[k]).sum())\n",
    "    else:\n",
    "        print(\" param:\", k, \"not found in saved model keys\")\n",
    "\n",
    "print(\"\\n=== DIAGNOSTIC COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ae9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused training code: you do not need to use the following code and functions. I'm putting them there as back-up options.\n",
    "\n",
    "\n",
    "# class WeightedTrainer(Trainer):\n",
    "#     def __init__(self, *args, classWeights=None, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.classWeights = classWeights  # keep CPU tensor\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits.float()  # ensure float32\n",
    "#         weight = self.classWeights.to(logits.device).to(logits.dtype)\n",
    "#         lossFct = nn.CrossEntropyLoss(weight=weight, ignore_index=-100)\n",
    "#         loss = lossFct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# trainingArgs = TrainingArguments(\n",
    "#     output_dir=\"./softNERModel\",\n",
    "    \n",
    "#     # Core hyperparameters\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=1e-6,\n",
    "#     max_grad_norm = 1.0,\n",
    "#     per_device_train_batch_size=batchSize,\n",
    "    \n",
    "#     # Regularization\n",
    "#     weight_decay=0.01,\n",
    "#     warmup_ratio=0.1,\n",
    "    \n",
    "#     save_strategy=\"steps\",\n",
    "    \n",
    "#     # Logging\n",
    "#     save_steps=500,\n",
    "#     logging_steps=100,\n",
    "    \n",
    "#     # Performance\n",
    "#     fp16=False,  # Set False if you get errors\n",
    "# )\n",
    "\n",
    "# trainer = WeightedTrainer(\n",
    "#     model=model,\n",
    "#     args=trainingArgs,\n",
    "#     train_dataset=tokenizedTrainingData,\n",
    "#     data_collator=dataCollator,\n",
    "#     classWeights=classWeights\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astext2025-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
